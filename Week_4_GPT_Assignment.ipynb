{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Language Model Implementation with Transformer Decoder\n",
        "\n",
        "This notebook demonstrates the implementation of a simple language model using a Transformer decoder architecture. Here's a summary of the key components and steps:\n",
        "\n",
        "1. Installation and Imports  \n",
        "Installs necessary libraries like torchdata, portalocker, and torchtext.  \n",
        "Imports required modules from PyTorch and other libraries.  \n",
        "\n",
        "2. Data Preparation  \n",
        "Implements a custom Tokenizer class to process text data.  \n",
        "Builds vocabulary and converts text to token IDs.  \n",
        "Prepares input sequences and labels for training.  \n",
        "\n",
        "3. Model Architecture  \n",
        "Implements key components of the Transformer architecture:  \n",
        "- Multi-Head Attention (MHMA)  \n",
        "- Feed-Forward Network (FFN)  \n",
        "- Positional Encoding  \n",
        "- Embedding layer  \n",
        "Combines these components into a Decoder layer and full Decoder model.  \n",
        "\n",
        "4. Training  \n",
        "Defines loss function (CrossEntropyLoss) and optimizer (SGD).  \n",
        "Implements a training loop that runs for 10,000 epochs.  \n",
        "\n",
        "5. Text Generation  \n",
        "Implements a generate function for text generation using the trained model.  \n",
        "Demonstrates text generation with different prompts.  \n",
        "\n",
        "6. Results  \n",
        "Shows that the model has memorized sentences from the training set.  \n",
        "Generates text based on given prompts, reproducing training sentences accurately.  \n",
        "\n",
        "The notebook effectively demonstrates the process of building, training, and using a simple language model based on the Transformer architecture for text generation tasks.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hdKopIVVtzKx"
      },
      "source": [
        "# Installation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "HF0ap-B02CW7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torchdata==0.6.0 in c:\\users\\balam\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.6.0)\n",
            "Requirement already satisfied: urllib3>=1.25 in c:\\users\\balam\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torchdata==0.6.0) (1.26.13)\n",
            "Requirement already satisfied: requests in c:\\users\\balam\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torchdata==0.6.0) (2.28.1)\n",
            "Requirement already satisfied: torch==2.0.0 in c:\\users\\balam\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torchdata==0.6.0) (2.0.0)\n",
            "Requirement already satisfied: filelock in c:\\users\\balam\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch==2.0.0->torchdata==0.6.0) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions in c:\\users\\balam\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch==2.0.0->torchdata==0.6.0) (4.12.2)\n",
            "Requirement already satisfied: sympy in c:\\users\\balam\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch==2.0.0->torchdata==0.6.0) (1.13.3)\n",
            "Requirement already satisfied: networkx in c:\\users\\balam\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch==2.0.0->torchdata==0.6.0) (2.8.8)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\balam\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch==2.0.0->torchdata==0.6.0) (3.1.2)\n",
            "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\balam\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->torchdata==0.6.0) (2.1.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\balam\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->torchdata==0.6.0) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\balam\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->torchdata==0.6.0) (2024.8.30)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\balam\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jinja2->torch==2.0.0->torchdata==0.6.0) (2.1.3)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\balam\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sympy->torch==2.0.0->torchdata==0.6.0) (1.3.0)\n",
            "Requirement already satisfied: portalocker==2.0.0 in c:\\users\\balam\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (2.0.0)\n",
            "Requirement already satisfied: pywin32!=226 in c:\\users\\balam\\appdata\\roaming\\python\\python310\\site-packages (from portalocker==2.0.0) (306)\n"
          ]
        }
      ],
      "source": [
        "!pip install torchdata==0.6.0\n",
        "!pip install portalocker==2.0.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "37SOAt7OE8pW"
      },
      "source": [
        "* See [here](https://github.com/pytorch/text) for compatability"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "amMg7VeGEnui"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torchtext==0.15.1 in c:\\users\\balam\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.15.1)\n",
            "Requirement already satisfied: tqdm in c:\\users\\balam\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torchtext==0.15.1) (4.64.1)\n",
            "Requirement already satisfied: requests in c:\\users\\balam\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torchtext==0.15.1) (2.28.1)\n",
            "Requirement already satisfied: torch==2.0.0 in c:\\users\\balam\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torchtext==0.15.1) (2.0.0)\n",
            "Requirement already satisfied: numpy in c:\\users\\balam\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torchtext==0.15.1) (1.26.0)\n",
            "Requirement already satisfied: torchdata==0.6.0 in c:\\users\\balam\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torchtext==0.15.1) (0.6.0)\n",
            "Requirement already satisfied: filelock in c:\\users\\balam\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch==2.0.0->torchtext==0.15.1) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions in c:\\users\\balam\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch==2.0.0->torchtext==0.15.1) (4.12.2)\n",
            "Requirement already satisfied: sympy in c:\\users\\balam\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch==2.0.0->torchtext==0.15.1) (1.13.3)\n",
            "Requirement already satisfied: networkx in c:\\users\\balam\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch==2.0.0->torchtext==0.15.1) (2.8.8)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\balam\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch==2.0.0->torchtext==0.15.1) (3.1.2)\n",
            "Requirement already satisfied: urllib3>=1.25 in c:\\users\\balam\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torchdata==0.6.0->torchtext==0.15.1) (1.26.13)\n",
            "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\balam\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->torchtext==0.15.1) (2.1.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\balam\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->torchtext==0.15.1) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\balam\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->torchtext==0.15.1) (2024.8.30)\n",
            "Requirement already satisfied: colorama in c:\\users\\balam\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tqdm->torchtext==0.15.1) (0.4.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\balam\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jinja2->torch==2.0.0->torchtext==0.15.1) (2.1.3)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\balam\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sympy->torch==2.0.0->torchtext==0.15.1) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install -U torchtext==0.15.1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OFnTb-YGtudi"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "NCbW1AUlZsoO"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import Tensor\n",
        "\n",
        "import torch.nn as nn\n",
        "from torch.nn import Parameter\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.functional import one_hot\n",
        "\n",
        "import torch.optim as optim\n",
        "\n",
        "#text lib\n",
        "import torchtext\n",
        "\n",
        "# tokenizer\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "\n",
        "#build vocabulary\n",
        "from torchtext.vocab import vocab\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "\n",
        "# get input_ids (numericalization)\n",
        "from torchtext.transforms import VocabTransform\n",
        "\n",
        "# get embeddings\n",
        "from torch.nn import Embedding\n",
        "\n",
        "from  pprint import pprint\n",
        "from yaml import safe_load\n",
        "import copy\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting nltk\n",
            "  Downloading nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: click in c:\\users\\balam\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk) (8.1.5)\n",
            "Requirement already satisfied: joblib in c:\\users\\balam\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk) (1.2.0)\n",
            "Collecting regex>=2021.8.3 (from nltk)\n",
            "  Downloading regex-2024.11.6-cp310-cp310-win_amd64.whl.metadata (41 kB)\n",
            "Requirement already satisfied: tqdm in c:\\users\\balam\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk) (4.64.1)\n",
            "Requirement already satisfied: colorama in c:\\users\\balam\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from click->nltk) (0.4.6)\n",
            "Downloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
            "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
            "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
            "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
            "   ------------- -------------------------- 0.5/1.5 MB 1.7 MB/s eta 0:00:01\n",
            "   ---------------------------------- ----- 1.3/1.5 MB 2.6 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 1.5/1.5 MB 2.5 MB/s eta 0:00:00\n",
            "Downloading regex-2024.11.6-cp310-cp310-win_amd64.whl (274 kB)\n",
            "Installing collected packages: regex, nltk\n",
            "Successfully installed nltk-3.9.1 regex-2024.11.6\n"
          ]
        }
      ],
      "source": [
        "!pip install nltk "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "IjjPjidG6kr1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\BalaM\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4wTjUVQRaoqI"
      },
      "source": [
        "# Load the dataset for LM modeling\n",
        "\n",
        " * We use a simple tokenizer and put"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "-aWB84SAN00b"
      },
      "outputs": [],
      "source": [
        "batch_size = 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "EBzIVx8u5VQU"
      },
      "outputs": [],
      "source": [
        "class Tokenizer(object):\n",
        "\n",
        "  def __init__(self,text):\n",
        "    self.text = text\n",
        "    self.word_tokenizer = get_tokenizer(tokenizer=\"basic_english\",language='en')\n",
        "    self.vocab_size = None\n",
        "\n",
        "  def get_tokens(self):\n",
        "    for sentence in self.text.strip().split('\\n'):\n",
        "      yield self.word_tokenizer(sentence)\n",
        "\n",
        "  def build_vocab(self):\n",
        "    v = build_vocab_from_iterator(self.get_tokens(),\n",
        "                                  min_freq=1,specials=['<unk>','<start>','<end>'])\n",
        "    v.set_default_index(v['<unk>']) # index of OOV\n",
        "    self.vocab_size = len(v)\n",
        "    return v\n",
        "\n",
        "  def token_ids(self):\n",
        "    v = self.build_vocab()\n",
        "    vt = VocabTransform(v)\n",
        "    num_tokens = len(self.word_tokenizer(self.text))\n",
        "    max_seq_len = np.ceil(num_tokens/batch_size)\n",
        "    data = torch.zeros(size=(1,num_tokens))\n",
        "    data = vt(self.word_tokenizer(self.text))\n",
        "    data = torch.tensor(data,dtype=torch.int64)\n",
        "    return data.reshape(batch_size,torch.tensor(max_seq_len,dtype=torch.int64))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "FE1L07Z-AoNz"
      },
      "outputs": [],
      "source": [
        "text = \"\"\"Best known for the invention of Error Correcting Codes, he was a true polymath who applied his mathematical and problem-solving skills to numerous disciplines.\n",
        "Reflecting on the significant benefits I received from Hamming, I decided to develop a tribute to his legacy. There has not been a previous biography of Hamming, and the few articles about him restate known facts and assumptions and leave us with open questions.\n",
        "One thought drove me as I developed this legacy project: An individual's legacy is more than a list of their attempts and accomplishments. Their tribute should also reveal the succeeding generations they inspired and enabled and what each attempted and achieved.\n",
        "This book is a unique genre containing my version of a biography that intertwines the story \"of a life\" and a multi-player memoir with particular events and turning points recalled by those, including me, who he inspired and enabled.\n",
        "Five years of research uncovered the people, places, opportunities, events, and influences that shaped Hamming. I discovered unpublished information, stories, photographs, videos, and personal remembrances to chronicle his life, which helped me put Hamming's\n",
        "legacy in the context I wanted.The result demonstrates many exceptional qualities, including his noble pursuit of excellence and helping others. Hamming paid attention to the details, his writings continue to influence, and his guidance is a timeless gift to the world.\n",
        "This biography is part of \"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "-OD9gBdaBHs1"
      },
      "outputs": [],
      "source": [
        "Tk = Tokenizer(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VTz7eGsSFK24",
        "outputId": "9022c4b6-73d7-4b04-e3a0-1b1acd1a724f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([10, 26])\n"
          ]
        }
      ],
      "source": [
        "x_raw = Tk.token_ids()\n",
        "print(x_raw.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ToL6j7ECOEIC",
        "outputId": "762685cc-50fa-45f6-cfb6-36983e93e4eb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['<unk>', '<start>', '<end>', ',', 'and', '.', 'the', 'a', 'of', 'to']\n"
          ]
        }
      ],
      "source": [
        "# let us display the first 10 tokens of the vocabulary\n",
        "v = Tk.build_vocab()\n",
        "pprint(v.vocab.get_itos()[0:10])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gs00A32ieNJp"
      },
      "source": [
        "* Create the input_ids and Labels from the raw input sequence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4dCg4PYfaXhP",
        "outputId": "63593f83-dc1e-4bcf-b0d6-105a4f29762d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<start> best known for the invention of error correcting codes , he was a true polymath who applied his mathematical and problem-solving skills to numerous disciplines . <end>\n"
          ]
        }
      ],
      "source": [
        "bs,raw_seq_len = x_raw.shape\n",
        "x = torch.empty(size=(bs,raw_seq_len+2),dtype=torch.int64)\n",
        "x[:,1:-1] =x_raw\n",
        "\n",
        "# insert the index of special tokens\n",
        "x[:,0] = torch.full(size=(1,batch_size),fill_value=v.vocab.get_stoi()['<start>'])\n",
        "x[:,-1] = torch.full(size=(1,batch_size),fill_value=v.vocab.get_stoi()['<end>'])\n",
        "\n",
        "#Quickly check implem\n",
        "v = Tk.build_vocab()\n",
        "words = []\n",
        "for idx in x[0,:]:\n",
        "  words.append(v.vocab.get_itos()[idx.item()])\n",
        "print(' '.join(words))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "A-Y0R3FSfEex"
      },
      "outputs": [],
      "source": [
        "# labels are just the input_ids shifted by right\n",
        "bs,seq_len = x.shape\n",
        "y = torch.empty(size=(bs,seq_len),dtype=torch.int64)\n",
        "y[:,0:-1] = copy.deepcopy(x[:,1:])\n",
        "\n",
        "#ignore the index of padded tokens while computing loss\n",
        "y[:,-1] = torch.full(size=(1,batch_size),fill_value=-100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z8-18UlZ2ES7"
      },
      "source": [
        "# Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "2kgKn1Emakah"
      },
      "outputs": [],
      "source": [
        "vocab_size = Tk.vocab_size\n",
        "seq_len = x.shape[1]\n",
        "embed_dim = 32\n",
        "dmodel = embed_dim\n",
        "dq = torch.tensor(4)\n",
        "dk = torch.tensor(4)\n",
        "dv = torch.tensor(4)\n",
        "heads = torch.tensor(8)\n",
        "d_ff = 4*dmodel"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oyQlw_Q9OLFT"
      },
      "source": [
        "* Define all the sub-layers (mhma,ffn) in the transformer blocks\n",
        "* Seed for $W_Q,W_K,W_V,W_O$, 43, 44 and 45, 46, respectively\n",
        "* Seed for ffn $W_1,W_2$,  47 and 48. There are no biases\n",
        "* Seed for output layer 49"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "Eg589leczupJ"
      },
      "outputs": [],
      "source": [
        "class MHMA(nn.Module):\n",
        "    def __init__(self, dmodel, dq, dk, dv, heads, mask=None):\n",
        "        super(MHMA, self).__init__()\n",
        "        self.heads = heads\n",
        "        self.dmodel = dmodel\n",
        "        self.dk = dk\n",
        "        self.dv = dv\n",
        "        self.Wq = nn.Linear(dmodel, dq * heads)\n",
        "        self.Wk = nn.Linear(dmodel, dk * heads)\n",
        "        self.Wv = nn.Linear(dmodel, dv * heads)\n",
        "        self.linear = nn.Linear(dv * heads, dmodel)\n",
        "        self.mask = mask\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size, seq_len, _ = x.size()\n",
        "        q = self.Wq(x).view(batch_size, seq_len, self.heads, self.dk).transpose(1, 2)\n",
        "        k = self.Wk(x).view(batch_size, seq_len, self.heads, self.dk).transpose(1, 2)\n",
        "        v = self.Wv(x).view(batch_size, seq_len, self.heads, self.dv).transpose(1, 2)\n",
        "\n",
        "        attn_scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.dk)\n",
        "        if self.mask is not None:\n",
        "            attn_scores = attn_scores.masked_fill(self.mask == 0, -1e9)\n",
        "        attn_weights = F.softmax(attn_scores, dim=-1)\n",
        "\n",
        "        attn_output = torch.matmul(attn_weights, v).transpose(1, 2).contiguous().view(batch_size, seq_len, self.heads * self.dv)\n",
        "        out = self.linear(attn_output)\n",
        "        return out\n",
        "\n",
        "class FFN(nn.Module):\n",
        "    def __init__(self, dmodel, d_ff):\n",
        "        super(FFN, self).__init__()\n",
        "        self.linear1 = nn.Linear(dmodel, d_ff)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.linear2 = nn.Linear(d_ff, dmodel)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.linear1(x)\n",
        "        out = self.relu(out)\n",
        "        out = self.linear2(out)\n",
        "        return out\n",
        "\n",
        "class PredictionHead(nn.Module):\n",
        "    def __init__(self, dmodel, vocab_size):\n",
        "        super(PredictionHead, self).__init__()\n",
        "        self.linear = nn.Linear(dmodel, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.linear(x)\n",
        "        return out\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, dmodel, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        pe = torch.zeros(max_len, dmodel)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, dmodel, 2).float() * (-math.log(10000.0) / dmodel))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.pe[:, :x.size(1), :]\n",
        "        return x\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "-Z_eWhYUz4rQ"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, dmodel, dq, dk, dv, d_ff, heads, mask=None):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "        self.mhma = MHMA(dmodel, dq, dk, dv, heads, mask=None)\n",
        "        self.layer_norm_1 = torch.nn.LayerNorm(dmodel)\n",
        "        self.layer_norm_2 = torch.nn.LayerNorm(dmodel)\n",
        "        self.ffn = FFN(dmodel, d_ff)\n",
        "\n",
        "    def forward(self, dec_rep):\n",
        "        attn_output = self.mhma(dec_rep)\n",
        "        attn_output = self.layer_norm_1(attn_output + dec_rep)\n",
        "        ffn_output = self.ffn(attn_output)\n",
        "        out = self.layer_norm_2(ffn_output + attn_output)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        " # Vocabulary (a dictionary with 'stoi' and 'itos' mappings)\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "class Embed(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim):\n",
        "        super(Embed, self).__init__()\n",
        "        self.embed = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.pe = PositionalEncoding(embed_dim)\n",
        "\n",
        "    def forward(self,x):#-\n",
        "        out = self.pe(self.embed(x))\n",
        "        return out#-\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "2yma_Vbc0k9n"
      },
      "outputs": [],
      "source": [
        "class Decoder(nn.Module):\n",
        "\n",
        "  def __init__(self,vocab_size,dmodel,dq,dk,dv,d_ff,heads,mask,num_layers=1):\n",
        "    super(Decoder,self).__init__()\n",
        "    self.embed_lookup = Embed(vocab_size,embed_dim)\n",
        "    self.dec_layers = nn.ModuleList(copy.deepcopy(DecoderLayer(dmodel,dq,dk,dv,d_ff,heads,mask)) for i in range(num_layers))\n",
        "    self.predict = PredictionHead(dmodel,vocab_size)\n",
        "\n",
        "  def forward(self,input_ids):\n",
        "    out = self.embed_lookup(input_ids)\n",
        "    for dec_layer in self.dec_layers:\n",
        "      out = dec_layer(out)\n",
        "    out = self.predict(out)\n",
        "\n",
        "    return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [],
      "source": [
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "M62_C8zS06ya"
      },
      "outputs": [],
      "source": [
        "model = Decoder(vocab_size,dmodel,dq,dk,dv,d_ff,heads,mask=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "n4y0aNcr2j1e"
      },
      "outputs": [],
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "6uhXU0ae2rxU"
      },
      "outputs": [],
      "source": [
        "def train(input_ids,labels,epochs=1000):\n",
        "  loss_trace = []\n",
        "  for epoch in range(epochs):\n",
        "    out = model(input_ids)\n",
        "    loss = criterion(out.view(-1, out.size(-1)), labels.view(-1))\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    optimizer.zero_grad()\n",
        "    loss_trace.append(loss.item())\n",
        "    if epoch % 100 == 0:\n",
        "        print(f\"Epoch {epoch}, Loss: {loss.item()}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "sHKeW38t2w4p"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0, Loss: 5.224754810333252\n",
            "Epoch 100, Loss: 4.919401168823242\n",
            "Epoch 200, Loss: 4.673630237579346\n",
            "Epoch 300, Loss: 4.4935302734375\n",
            "Epoch 400, Loss: 4.3542914390563965\n",
            "Epoch 500, Loss: 4.227426528930664\n",
            "Epoch 600, Loss: 4.097816467285156\n",
            "Epoch 700, Loss: 3.96120285987854\n",
            "Epoch 800, Loss: 3.8177390098571777\n",
            "Epoch 900, Loss: 3.6685128211975098\n",
            "Epoch 1000, Loss: 3.515084743499756\n",
            "Epoch 1100, Loss: 3.35856294631958\n",
            "Epoch 1200, Loss: 3.2001712322235107\n",
            "Epoch 1300, Loss: 3.0410921573638916\n",
            "Epoch 1400, Loss: 2.8822875022888184\n",
            "Epoch 1500, Loss: 2.724494218826294\n",
            "Epoch 1600, Loss: 2.5684170722961426\n",
            "Epoch 1700, Loss: 2.4146533012390137\n",
            "Epoch 1800, Loss: 2.2636215686798096\n",
            "Epoch 1900, Loss: 2.1160006523132324\n",
            "Epoch 2000, Loss: 1.972361445426941\n",
            "Epoch 2100, Loss: 1.8333595991134644\n",
            "Epoch 2200, Loss: 1.6995997428894043\n",
            "Epoch 2300, Loss: 1.5718235969543457\n",
            "Epoch 2400, Loss: 1.4505717754364014\n",
            "Epoch 2500, Loss: 1.336242437362671\n",
            "Epoch 2600, Loss: 1.2293504476547241\n",
            "Epoch 2700, Loss: 1.1299649477005005\n",
            "Epoch 2800, Loss: 1.0380232334136963\n",
            "Epoch 2900, Loss: 0.9532957673072815\n",
            "Epoch 3000, Loss: 0.8752129077911377\n",
            "Epoch 3100, Loss: 0.8038969039916992\n",
            "Epoch 3200, Loss: 0.7385310530662537\n",
            "Epoch 3300, Loss: 0.6785522103309631\n",
            "Epoch 3400, Loss: 0.6235127449035645\n",
            "Epoch 3500, Loss: 0.5729107856750488\n",
            "Epoch 3600, Loss: 0.5267035365104675\n",
            "Epoch 3700, Loss: 0.4849294126033783\n",
            "Epoch 3800, Loss: 0.44724658131599426\n",
            "Epoch 3900, Loss: 0.41319409012794495\n",
            "Epoch 4000, Loss: 0.38231804966926575\n",
            "Epoch 4100, Loss: 0.3541437089443207\n",
            "Epoch 4200, Loss: 0.3282119035720825\n",
            "Epoch 4300, Loss: 0.3042427599430084\n",
            "Epoch 4400, Loss: 0.281877726316452\n",
            "Epoch 4500, Loss: 0.2607966363430023\n",
            "Epoch 4600, Loss: 0.24066585302352905\n",
            "Epoch 4700, Loss: 0.22127491235733032\n",
            "Epoch 4800, Loss: 0.20272448658943176\n",
            "Epoch 4900, Loss: 0.18531359732151031\n",
            "Epoch 5000, Loss: 0.16978202760219574\n",
            "Epoch 5100, Loss: 0.1567482054233551\n",
            "Epoch 5200, Loss: 0.14567777514457703\n",
            "Epoch 5300, Loss: 0.13596247136592865\n",
            "Epoch 5400, Loss: 0.12728099524974823\n",
            "Epoch 5500, Loss: 0.11946671456098557\n",
            "Epoch 5600, Loss: 0.11238263547420502\n",
            "Epoch 5700, Loss: 0.10595697909593582\n",
            "Epoch 5800, Loss: 0.10010446608066559\n",
            "Epoch 5900, Loss: 0.09475983679294586\n",
            "Epoch 6000, Loss: 0.08986733108758926\n",
            "Epoch 6100, Loss: 0.0853690356016159\n",
            "Epoch 6200, Loss: 0.08122866600751877\n",
            "Epoch 6300, Loss: 0.07740602642297745\n",
            "Epoch 6400, Loss: 0.07386764883995056\n",
            "Epoch 6500, Loss: 0.0705891102552414\n",
            "Epoch 6600, Loss: 0.06754478067159653\n",
            "Epoch 6700, Loss: 0.06471267342567444\n",
            "Epoch 6800, Loss: 0.06207360327243805\n",
            "Epoch 6900, Loss: 0.05961088836193085\n",
            "Epoch 7000, Loss: 0.05730646848678589\n",
            "Epoch 7100, Loss: 0.05514875054359436\n",
            "Epoch 7200, Loss: 0.05312420427799225\n",
            "Epoch 7300, Loss: 0.051223184913396835\n",
            "Epoch 7400, Loss: 0.04943401739001274\n",
            "Epoch 7500, Loss: 0.04774590581655502\n",
            "Epoch 7600, Loss: 0.04615722596645355\n",
            "Epoch 7700, Loss: 0.044656697660684586\n",
            "Epoch 7800, Loss: 0.04323747009038925\n",
            "Epoch 7900, Loss: 0.04189397767186165\n",
            "Epoch 8000, Loss: 0.040619734674692154\n",
            "Epoch 8100, Loss: 0.039413001388311386\n",
            "Epoch 8200, Loss: 0.038266874849796295\n",
            "Epoch 8300, Loss: 0.037176910787820816\n",
            "Epoch 8400, Loss: 0.036140572279691696\n",
            "Epoch 8500, Loss: 0.03515421226620674\n",
            "Epoch 8600, Loss: 0.03421440348029137\n",
            "Epoch 8700, Loss: 0.033316902816295624\n",
            "Epoch 8800, Loss: 0.03245994448661804\n",
            "Epoch 8900, Loss: 0.031640876084566116\n",
            "Epoch 9000, Loss: 0.030857987701892853\n",
            "Epoch 9100, Loss: 0.030109381303191185\n",
            "Epoch 9200, Loss: 0.02939300797879696\n",
            "Epoch 9300, Loss: 0.028706364333629608\n",
            "Epoch 9400, Loss: 0.028048107400536537\n",
            "Epoch 9500, Loss: 0.027416473254561424\n",
            "Epoch 9600, Loss: 0.026808924973011017\n",
            "Epoch 9700, Loss: 0.026224983856081963\n",
            "Epoch 9800, Loss: 0.02566378191113472\n",
            "Epoch 9900, Loss: 0.02512393519282341\n"
          ]
        }
      ],
      "source": [
        "# run the model for 10K epochs\n",
        "train(x,y,10000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J9Sti5BtU03d"
      },
      "source": [
        "The loss is about 0.09 after 10K epochs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sSHVHX4J40eS"
      },
      "source": [
        "# Generate text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {},
      "outputs": [],
      "source": [
        "@torch.inference_mode()\n",
        "def generate(model, prompt, max_words=25):\n",
        "    \"\"\"\n",
        "    Generate text using a model and a vocabulary.\n",
        "\n",
        "    Args:\n",
        "        model: The trained model.\n",
        "        v: Vocabulary object (from Tk.build_vocab()).\n",
        "        prompt: Initial list of tokens (list of strings).\n",
        "        max_words: Maximum number of words to generate.\n",
        "\n",
        "    Returns:\n",
        "        Generated text (string).\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    device = next(model.parameters()).device\n",
        "\n",
        "    # Get stoi (string-to-index) and itos (index-to-string) mappings\n",
        "    stoi = v.vocab.get_stoi()\n",
        "    itos = v.vocab.get_itos()\n",
        "\n",
        "    # Tokenize the prompt (convert list of tokens to indices)\n",
        "    tokens = torch.tensor(\n",
        "        [stoi.get(token, stoi['<unk>']) for token in prompt],\n",
        "        dtype=torch.long, device=device\n",
        "    )\n",
        "\n",
        "    for _ in range(max_words):\n",
        "        # Forward pass through the model\n",
        "        out = model(tokens.unsqueeze(0))  # Shape: (1, seq_len, vocab_size)\n",
        "\n",
        "        # Get the logits for the last token\n",
        "        logits = out[:, -1, :]  # Shape: (1, vocab_size)\n",
        "\n",
        "        # Sample the next token\n",
        "        next_token = torch.multinomial(logits.softmax(dim=-1), 1).item()\n",
        "\n",
        "        # Stop generation if the '<end>' token is generated\n",
        "        if itos[next_token] == '<end>':\n",
        "            break\n",
        "\n",
        "        # Append the new token to the sequence\n",
        "        tokens = torch.cat((tokens, torch.tensor([next_token], device=device)))\n",
        "\n",
        "    # Detokenize the sequence (convert indices back to tokens)\n",
        "    generated_text = ' '.join([itos[token.item()] for token in tokens])\n",
        "    return generated_text\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'<start> accomplishments . this legacy inspired and enabled and personal remembrances'"
            ]
          },
          "execution_count": 82,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "generate(model,prompt=['<start>'],max_words=10)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "xcA1Lu4j6Eq-",
        "outputId": "85b35deb-faf3-4532-e01c-1fded4423014"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'<unk> s <unk> a <unk> <unk> <unk> and assumptions and a thought drove me . this book the world . . this biography that shaped hamming .'"
            ]
          },
          "execution_count": 79,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "generate(model,prompt='<start>',max_words=25)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sTaqoJ_2VDDa"
      },
      "source": [
        "* Note the model has memorized the sentence from the training set. Given the start token, if your implementation reproduce a sentence as is in the training set, then your implementation is likely to be correct.\n",
        "* Suppose the prompt is `<start> best known`, then we expect the model to produce the first sentence as is"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'<start> best known for the invention of research uncovered the result particular events and achieved . . this book is a helping others . hamming .'"
            ]
          },
          "execution_count": 70,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "generate(model,prompt=['<start>','best','known'],max_words=25)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "2N_Shu7TXjAT",
        "outputId": "c2ae2e3d-88c0-4d97-bb3d-572055d7fff7"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'for the invention of error correcting codes , he was a true polymath who applied his mathematical and problem-solving skills to numerous disciplines'"
            ]
          },
          "execution_count": 101,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "generate(model,prompt=['<start>','best','known'],max_words=25)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P1hK47T5X2fY"
      },
      "source": [
        "* Change the prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'<start> reflecting on the significant and enabled and enabled and turning points recalled by those timeless gift to details of and helping others . one thought drove me'"
            ]
          },
          "execution_count": 73,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "generate(model,prompt=['<start>','reflecting','on'],max_words=25)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Q6_WM1K2X7qZ",
        "outputId": "bb78ddc5-2efa-4830-ac4b-2a359de9f941"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'the significant benefits i received from hamming , i decided to develop a tribute to his legacy . there has not been a'"
            ]
          },
          "execution_count": 102,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "generate(model,prompt=['<start>','reflecting','on'],max_words=25)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
